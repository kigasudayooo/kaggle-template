# Kaggle Competition - Claude Code向け汎用ガイド

最終更新: 2025-12-12

このドキュメントは、Pythonを使用したKaggleコンペティション全般で有用なClaude Code向けの汎用的なガイドラインを提供します。

---

## 目次

1. [TODO管理の重要性](#todo管理の重要性)
2. [環境構築（uv使用）](#環境構築uv使用)
3. [実験管理（Trackio）の必須使用](#実験管理trackioの必須使用)
4. [プロジェクト構造](#プロジェクト構造)
5. [Kaggle提出用ノートブックの制約](#kaggle提出用ノートブックの制約)
6. [コーディング規約](#コーディング規約)
7. [Git運用ルール](#git運用ルール)
8. [パフォーマンス最適化](#パフォーマンス最適化)
9. [デバッグ・トラブルシューティング](#デバッグトラブルシューティング)

---

## TODO管理の重要性

**原則**: すべての作業前にtodo.mdを更新すること

Kaggleコンペティションでは環境を頻繁に切り替えて作業するため、あらゆる作業発生時に必ずtodo.mdを更新してください。

### 絶対ルール: 作業前のTODO更新

```
間違ったワークフロー:
1. 作業を開始する
2. 作業を進める
3. （忘れた頃に）todo.mdを更新する

正しいワークフロー:
1. todo.mdを開く
2. これから行う作業をtodo.mdに追加/更新
3. Commit: "docs: Add [task] to TODO"
4. 作業を開始する
5. 作業完了後、即座にtodo.mdで完了マーク
6. Commit: "docs: Mark [task] as completed"
```

### todo.md更新が必要な作業トリガー（必須）

以下のいずれかの作業が発生した場合、**作業開始前に必ずtodo.mdを更新**:

1. **ファイル作成・編集**: 新しいスクリプト作成、既存コード修正
2. **実験・訓練**: 新しいモデル訓練、ハイパーパラメータ変更
3. **デバッグ・問題解決**: エラー発生、バグ発見
4. **データ処理・分析**: 前処理、EDA、統計分析
5. **推論・提出**: 推論ノートブック作成、提出ファイル生成
6. **リファクタリング・整理**: コード整理、ファイル構成変更

### todo.mdの標準構造

```markdown
# TODO - [プロジェクト名]

最終更新: YYYY-MM-DD HH:MM

## 現在進行中

### [作業名] - [開始時刻]
- **状態**: 進行中
- **目的**: [この作業の目的]
- **進捗**:
  - [完了したステップ]
  - [進行中のステップ]
  - [未着手のステップ]
- **関連ファイル**: `path/to/file.py`

## 最近完了したタスク

### [完了日時] - [タスク名]
- **結果**: [何を達成したか]
- **学び**: [このタスクから得た教訓]
- **次のアクション**: [派生する次のステップ]

## 次のステップ（優先順位順）

### 1. [タスク名] 緊急度: 高/中/低
**目的**: [なぜこのタスクが重要か]
**手順**: [ステップの詳細]

## 発見された問題・バグ

### [問題名] - [発見日時]
- **症状**: [何が起きているか]
- **再現手順**: [問題を再現する方法]
- **仮説**: [原因の推測]
- **対応状況**: 未対応/調査中/修正中/完了

## アイデア・改善案

### [アイデア名]
- **概要**: [アイデアの説明]
- **期待効果**: [実装したら何が良くなるか]
- **優先度**: 高/中/低
```

---

## 環境構築（uv使用）

**重要原則**: KaggleプロジェクトではPythonパッケージ管理に**uvを使用すること**

### uvとは

**uv**は、Rustで書かれた超高速なPythonパッケージマネージャーです。

- **高速**: pipの10-100倍の速度
- **信頼性**: 依存関係の解決が確実
- **互換性**: pip、pip-tools、poetryと互換

### uvのインストール

```bash
# macOS / Linux
curl -LsSf https://astral.sh/uv/install.sh | sh

# Windows (PowerShell)
powershell -c "irm https://astral.sh/uv/install.ps1 | iex"

# Homebrew (macOS)
brew install uv
```

### プロジェクト初期化

```bash
# プロジェクトディレクトリ作成
mkdir kaggle-competition
cd kaggle-competition

# Git初期化
git init

# uvプロジェクト初期化
uv init

# Python バージョン指定（Kaggle環境に合わせる）
uv python install 3.10
uv python pin 3.10
uv sync
```

### パッケージ管理の基本

**正しい方法** :
```bash
# パッケージ追加（本番環境）
uv add torch torchvision
uv add pandas numpy scikit-learn
uv add timm pillow tqdm pyyaml

# 開発環境のみ
uv add --dev jupyter pytest black mypy

# 実験管理ツール
uv add trackio
```

**間違った方法**:
```bash
pip install torch              # 禁止
uv pip install torch           # 禁止
python -m pip install torch    # 禁止
```

### GPU対応PyTorch

```bash
# Kaggleと同じCUDA対応PyTorch
uv add torch torchvision --index-url https://download.pytorch.org/whl/cu121

# CPU版（テスト用のみ）
uv add torch torchvision --index-url https://download.pytorch.org/whl/cpu
```

### スクリプト実行

```bash
# uvで仮想環境を使ってスクリプト実行
uv run python train.py

# Jupyter起動
uv run jupyter lab
```

---

## 実験管理（Trackio）の必須使用

**原則**: すべての訓練時にtrackioを使用すること

### Trackioとは

**Trackio**は、Hugging Faceが開発した軽量・無料の実験管理ライブラリです。

- **Weights & Biases互換API**: wandbと同じインターフェース
- **ローカルファースト**: データはローカルのSQLiteに保存
- **Gradio Dashboard**: ブラウザで実験結果を可視化
- **無料**: クラウド費用なし、アカウント不要

### 基本的な使用方法

```python
import trackio

# 1. 実験の初期化
trackio.init(
    project="project-name",
    config={
        "model": "convnext_small",
        "img_size": 224,
        "batch_size": 32,
        "learning_rate": 0.0001,
        "epochs": 100,
    }
)

# 2. 訓練ループ内でメトリクスをログ
for epoch in range(num_epochs):
    train_loss, train_metrics = train_epoch(...)
    val_loss, val_metrics = validate(...)

    trackio.log({
        "epoch": epoch,
        "train/loss": train_loss,
        "train/metric": train_metrics,
        "val/loss": val_loss,
        "val/metric": val_metrics,
        "learning_rate": optimizer.param_groups[0]['lr'],
    })

# 3. 訓練終了時
trackio.finish()
```

### 訓練開始後の必須アクション

訓練開始直後に以下のコマンドを実行してDashboardを確認:

```bash
trackio show --project "project-name"
# ブラウザが自動的に開く: http://127.0.0.1:7860
```

---

## プロジェクト構造

### 推奨ディレクトリ構造

```
project/
├── data/                    # データファイル（gitignored）
│   ├── raw/                # 元データ（不変）
│   ├── processed/          # 前処理済み
│   └── sample/             # サンプルデータ（git tracked）
├── notebooks/              # Jupyter notebooks
│   ├── eda/               # 探索的データ分析
│   ├── experiments/       # 実験ノートブック
│   └── submission/        # Kaggle提出用
├── src/
│   ├── data/              # データセットクラス
│   ├── models/            # モデル定義
│   ├── features/          # 特徴量エンジニアリング
│   ├── utils/             # ユーティリティ
│   └── scripts/           # 訓練・推論スクリプト
├── configs/               # YAML設定ファイル
├── models/                # 訓練済みモデル（gitignored）
├── experiments/           # 実験出力（gitignored）
├── docs/                  # ドキュメント
├── todo.md                # TODO管理（重要）
├── CLAUDE.md              # このファイル
├── README.md              # プロジェクトREADME
└── pyproject.toml         # uv設定
```

### ファイル命名規則

**実験スクリプト**: `exp{XX}_{description}.py`
```
exp01_baseline.py
exp02_enhanced_augmentation.py
```

**実験出力**: `experiments/exp{XX}/`
```
experiments/exp01/
├── submission.csv
├── n001.png
└── experiment.log
```

---

## Kaggle提出用ノートブックの制約

### 必須要件

1. **モデル定義はノートブック内に完全定義** - 外部importは不可
2. **weights_only=False** - PyTorch 2.6+でtorch.load()に必須
3. **新しいパッケージは、アップロードして使う必要あり** - 標準ノートブックでは、pip install不可の場合があるので、自前で準備する必要。

### 標準ノートブック構造

```python
# 1. Imports
import pandas as pd
import torch
import timm
from torchvision import transforms
from PIL import Image

# 2. Model Definition（完全なクラス定義）
class YourModel(nn.Module):
    ...

# 3. Configuration
class CFG:
    device = torch.device('cuda')
    img_size = 224
    n_folds = 5

# 4. Transform（torchvisionのみ）
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                        std=[0.229, 0.224, 0.225])
])

# 5. Model Loading
checkpoint = torch.load(path, map_location=device, weights_only=False)
model.load_state_dict(checkpoint['model_state_dict'])

# 6. Inference
# 7. Create Submission
```

### 提出前チェックリスト

- [ ] albumentationsを使用していないか
- [ ] torchvision transformsを使用しているか
- [ ] モデルクラスがノートブック内で完全に定義されているか
- [ ] weights_only=Falseを指定しているか
- [ ] パスが/kaggle/input/...になっているか
- [ ] submission.csvが正しいフォーマットか

---

## コーディング規約

### Python Style

- PEP 8に従う
- すべての関数に型ヒント: `def func(x: int) -> str:`
- 公開関数にdocstring（Google style）
- 変数名: 説明的、最低3文字
- ループ以外で単一文字変数は禁止

### ML/DL特有のパターン

```python
# 再現性のためシードを設定
import random
import numpy as np
import torch

def set_seed(seed: int = 42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True

# 推論時はno_grad
with torch.no_grad():
    predictions = model(inputs)

# DataLoader設定
DataLoader(
    dataset,
    batch_size=32,
    shuffle=True,
    num_workers=4,
    pin_memory=True  # GPU訓練用
)
```

### 標準モデルラッパー

```python
import torch.nn as nn
import timm

class ImageModel(nn.Module):
    def __init__(self, model_name: str, num_classes: int, pretrained: bool = True):
        super().__init__()
        self.backbone = timm.create_model(
            model_name,
            pretrained=pretrained,
            num_classes=num_classes
        )
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.backbone(x)
```

### チェックポイント保存

```python
# メタデータ付きで保存
torch.save({
    'epoch': epoch,
    'model_state_dict': model.state_dict(),
    'optimizer_state_dict': optimizer.state_dict(),
    'loss': loss,
    'config': config,
}, f'models/checkpoint_fold{fold}_epoch{epoch}.pth')

# 読み込み
checkpoint = torch.load(
    checkpoint_path,
    map_location=device,
    weights_only=False  # PyTorch 2.6+で必須
)
model.load_state_dict(checkpoint['model_state_dict'])
```

---

## Git運用ルール

### ブランチ命名

- `feature/description` - 新機能
- `experiment/exp-name` - 実験
- `fix/bug-description` - バグ修正
- `docs/update-description` - ドキュメント

### コミットメッセージ（Conventional Commits）

```bash
feat: Add ConvNeXt training script
fix: Resolve checkpoint loading error
exp: Record 5-fold CV results (R²=0.58)
docs: Update TODO with experiment status
chore: Add dependency to pyproject.toml
```

### .gitignore必須項目

```gitignore
# Python
__pycache__/
*.py[cod]
.Python
*.egg-info/

# 仮想環境
.venv/
venv/

# IDE
.vscode/
.idea/

# Jupyter
.ipynb_checkpoints/

# データ（大容量）
data/raw/
data/processed/
*.csv
*.jpg
*.png

# モデル（大容量）
*.pth
*.pt
models/*/

# 環境変数
.env
```

### Pre-commit Checklist

- [ ] todo.mdが更新されている
- [ ] 大きなデータファイルが含まれていない
- [ ] コードがlintを通過: `uv run black src/`
- [ ] テストが通過: `uv run pytest`
- [ ] 機密データがコミットに含まれていない

---

## パフォーマンス最適化

### 訓練開始前の必須確認

```bash
# GPU情報を確認
nvidia-smi

# 確認項目:
# - GPU名とVRAM容量
# - 現在のVRAM使用量
# - GPU利用率（%）
```

### batch_sizeとnum_workersの最適化

| GPU | 画像サイズ | batch_size推奨 | num_workers推奨 |
|-----|-----------|---------------|-----------------|
| RTX 4090 (24GB) | 512×512 | 16-24 | 8-12 |
| RTX 4090 (24GB) | 224×224 | 32-48 | 8-12 |
| RTX 3090 (24GB) | 512×512 | 12-16 | 6-8 |
| RTX 3080 (10GB) | 512×512 | 6-8 | 4-6 |

### Mixed Precision Training

```python
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()

for batch in dataloader:
    optimizer.zero_grad()
    
    with autocast():
        outputs = model(inputs)
        loss = criterion(outputs, targets)
    
    scaler.scale(loss).backward()
    scaler.step(optimizer)
    scaler.update()
```

### Gradient Accumulation

```python
accumulation_steps = 4

for i, batch in enumerate(dataloader):
    outputs = model(inputs)
    loss = criterion(outputs, targets) / accumulation_steps
    loss.backward()
    
    if (i + 1) % accumulation_steps == 0:
        optimizer.step()
        optimizer.zero_grad()
```

### 並列処理

```python
from concurrent.futures import ProcessPoolExecutor
import multiprocessing

n_cores = multiprocessing.cpu_count()
max_workers = max(1, n_cores - 1)  # 1コアはシステム用に残す

with ProcessPoolExecutor(max_workers=max_workers) as executor:
    futures = {executor.submit(process_item, item): item for item in work_items}
    for future in tqdm(as_completed(futures), total=len(futures)):
        result = future.result()
```

---

## デバッグ・トラブルシューティング

### よくある問題

1. **CUDA OOM**: batch_sizeを減らす、gradient accumulationを使用
2. **訓練が遅い**: num_workers、pin_memory、mixed precisionを確認
3. **CV/LB相関が悪い**: データリーク確認、GroupKFold検討
4. **Kaggle提出エラー**: モデル定義がノートブック内にあるか確認
5. **Import errors**: ライブラリバージョンがKaggle環境と一致しているか確認

### 診断コマンド

```bash
# GPUメモリ確認
nvidia-smi

# 環境確認
uv run python -c "import torch; print(torch.__version__, torch.cuda.is_available())"
```

### Cross-Validation戦略

```python
from sklearn.model_selection import StratifiedKFold, GroupKFold

# 分類（ターゲットで層化）
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# 時系列・グループデータ（データリーク防止）
gkf = GroupKFold(n_splits=5)
```

**注意**: 不均衡データやグループ/時系列データには単純なKFoldを使用しない

---

## 重要な制約とベストプラクティス

### DO:
- [ ] API keysは環境変数で: `os.getenv('KAGGLE_KEY')`
- [ ] 認証情報は`.env`に（gitignored）
- [ ] すべての入力データの形状と型を検証
- [ ] ハイパーパラメータに設定ファイルを使用
- [ ] すべての実験をtrackioでログ
- [ ] 複雑なアルゴリズムにコメント
- [ ] データ処理関数にテストを書く

### DON'T:
- [ ] ファイルパスのハードコード
- [ ] APIキーやパスワードのコミット
- [ ] データ検証のスキップ
- [ ] ランダムシードなしの訓練
- [ ] 非推奨PyTorch APIの使用
- [ ] 提出ノートブックでalbumentationsの使用
- [ ] pip installの使用（uv addを使用）

---

## コード品質ツールチェーン

### 概要

プロジェクトの品質を維持するため、以下のツールチェーンを使用します：

| ツール | 用途 | 特徴 |
|--------|------|------|
| [Ruff](https://github.com/astral-sh/ruff) | Linter + Formatter | Rust製、超高速、Flake8/Black/isort統合 |
| [ty](https://github.com/astral-sh/ty) | 型チェッカー | Rust製、高速、Astral開発 |
| [import-linter](https://github.com/seddonym/import-linter) | アーキテクチャ検証 | モジュール間依存関係の強制 |
| [Task](https://taskfile.dev/) | タスクランナー | Makeの代替、YAML定義、クロスプラットフォーム |
| [pre-commit](https://pre-commit.com/) | Git hooks | コミット前の自動チェック |

---

### Ruff（Linter + Formatter）

[Ruff](https://docs.astral.sh/ruff/)はRust製の超高速Python linter/formatterです。Flake8、Black、isort、pyupgrade等を1つのツールに統合し、10-100倍高速に動作します。

#### インストール

```bash
uv add --dev ruff
```

#### pyproject.toml設定

```toml
[tool.ruff]
line-length = 88
target-version = "py310"
exclude = [
    ".git",
    ".venv",
    "__pycache__",
    "*.ipynb",
]

[tool.ruff.lint]
select = [
    "E",      # pycodestyle errors
    "F",      # Pyflakes
    "I",      # isort
    "N",      # pep8-naming
    "UP",     # pyupgrade
    "B",      # flake8-bugbear
    "C4",     # flake8-comprehensions
    "SIM",    # flake8-simplify
    "T20",    # flake8-print (debugのprint検出)
]
ignore = [
    "E501",   # line too long (formatterに任せる)
]

[tool.ruff.lint.per-file-ignores]
"tests/*" = ["S101"]  # assertの使用を許可
"notebooks/*" = ["T20", "E402"]  # print許可、import順序緩和

[tool.ruff.format]
quote-style = "double"
indent-style = "space"
skip-magic-trailing-comma = false
line-ending = "auto"
```

#### 使用方法

```bash
# Lintチェック
uv run ruff check src/

# Lintチェック + 自動修正
uv run ruff check --fix src/

# フォーマット
uv run ruff format src/

# フォーマットチェック（変更なし）
uv run ruff format --check src/
```

#### ベストプラクティス

1. **デフォルトから始める**: 最初は基本ルール（E, F, I）のみ有効化し、徐々に追加
2. **自動修正を活用**: `--fix`オプションで安全な修正を自動適用
3. **per-file-ignoresを活用**: テストやノートブックには異なるルールを適用
4. **CI/CDに統合**: pre-commitとGitHub Actionsで自動チェック

---

### ty（型チェッカー）

[ty](https://docs.astral.sh/ty/)はAstral社（Ruff/uv開発元）が開発中の超高速Python型チェッカーです。Rust製で、mypyの10-20倍、Pyrightの2-5倍高速です。

**注意**: tyは2025年12月現在プレビュー版です。本番環境では安定版のmypyも併用を検討してください。

#### インストール

```bash
uv add --dev ty

# または uvx で直接実行
uvx ty check
```

#### pyproject.toml設定

```toml
[tool.ty]
python-version = "3.10"
```

#### 使用方法

```bash
# 型チェック実行
uv run ty check

# 特定ディレクトリのみ
uv run ty check src/

# uvxで直接実行（インストール不要）
uvx ty check
```

#### 特徴

- **高速**: 大規模コードベースでも数秒で完了
- **詳細なエラーメッセージ**: 問題の原因と修正方法を説明
- **段階的導入**: 型注釈のないコードでも寛容に動作

#### mypy併用（安定版が必要な場合）

```bash
uv add --dev mypy

# mypy設定
[tool.mypy]
python_version = "3.10"
warn_return_any = true
warn_unused_ignores = true
disallow_untyped_defs = true
ignore_missing_imports = true
```

---

### import-linter（アーキテクチャ検証）

[import-linter](https://import-linter.readthedocs.io/)はPythonプロジェクトのモジュール間依存関係を強制するツールです。レイヤードアーキテクチャや循環依存の防止に有効です。

#### インストール

```bash
uv add --dev import-linter
```

#### pyproject.toml設定

```toml
[tool.importlinter]
root_packages = ["src"]
include_external_packages = true

# レイヤーコントラクト: 上位レイヤーは下位に依存可、逆は不可
[tool.importlinter.contracts.layers]
name = "Layered Architecture"
type = "layers"
layers = [
    "src.scripts",      # 最上位: 実行スクリプト
    "src.models",       # モデル定義
    "src.data",         # データセット
    "src.features",     # 特徴量エンジニアリング
    "src.utils",        # 最下位: ユーティリティ
]

# 独立性コントラクト: 相互import禁止
[tool.importlinter.contracts.model_independence]
name = "Models are independent"
type = "independence"
modules = [
    "src.models.baseline",
    "src.models.advanced",
    "src.models.ensemble",
]

# 禁止コントラクト: 特定のimportを禁止
[tool.importlinter.contracts.no_utils_to_models]
name = "Utils cannot import models"
type = "forbidden"
source_modules = ["src.utils"]
forbidden_modules = ["src.models"]
```

#### 使用方法

```bash
# アーキテクチャ検証
uv run lint-imports

# 詳細出力
uv run lint-imports --verbose
```

#### コントラクトタイプ

| タイプ | 用途 |
|--------|------|
| `layers` | レイヤードアーキテクチャ（上→下の依存のみ許可） |
| `independence` | モジュール間の相互import禁止 |
| `forbidden` | 特定のimportパターンを禁止 |

#### ベストプラクティス

1. **プロジェクト初期に導入**: 後からの導入は既存の違反修正が大変
2. **主要レイヤーを定義**: `scripts → models → data → utils`
3. **CI/CDで強制**: 違反があればビルド失敗

---

### Task（タスクランナー）

[Task](https://taskfile.dev/)はGo製のタスクランナーで、Makeの代替として設計されています。YAMLで定義し、クロスプラットフォームで動作します。

#### インストール

```bash
# macOS
brew install go-task

# Linux
sh -c "$(curl --location https://taskfile.dev/install.sh)" -- -d -b ~/.local/bin

# pip経由
pip install go-task-bin
```

#### Taskfile.yml設定

```yaml
version: '3'

vars:
  PYTHON: uv run python
  SRC_DIR: src
  TEST_DIR: tests

tasks:
  # デフォルトタスク
  default:
    desc: Show available tasks
    cmds:
      - task --list

  # 環境セットアップ
  setup:
    desc: Setup development environment
    cmds:
      - uv sync --all-extras --dev
      - uv run pre-commit install

  # Linting
  lint:
    desc: Run all linters
    cmds:
      - uv run ruff check {{.SRC_DIR}}
      - uv run ruff format --check {{.SRC_DIR}}

  lint:fix:
    desc: Fix linting issues
    cmds:
      - uv run ruff check --fix {{.SRC_DIR}}
      - uv run ruff format {{.SRC_DIR}}

  # 型チェック
  typecheck:
    desc: Run type checker
    cmds:
      - uvx ty check

  # アーキテクチャ検証
  arch:
    desc: Check architecture constraints
    cmds:
      - uv run lint-imports

  # テスト
  test:
    desc: Run tests
    cmds:
      - uv run pytest {{.TEST_DIR}} -v

  test:cov:
    desc: Run tests with coverage
    cmds:
      - uv run pytest {{.TEST_DIR}} --cov={{.SRC_DIR}} --cov-report=html

  # 全チェック（CI用）
  check:
    desc: Run all checks (lint, typecheck, arch, test)
    cmds:
      - task: lint
      - task: typecheck
      - task: arch
      - task: test

  # 訓練
  train:
    desc: Run training script
    cmds:
      - "{{.PYTHON}} {{.SRC_DIR}}/scripts/train.py {{.CLI_ARGS}}"

  # Jupyter
  jupyter:
    desc: Start Jupyter Lab
    cmds:
      - uv run jupyter lab

  # クリーンアップ
  clean:
    desc: Clean generated files
    cmds:
      - rm -rf __pycache__ .pytest_cache .ruff_cache
      - find . -type d -name "__pycache__" -exec rm -rf {} + 2>/dev/null || true
```

#### 使用方法

```bash
# タスク一覧表示
task

# 特定タスク実行
task lint
task test
task check

# 引数付き実行
task train -- --config configs/baseline.yaml

# 複数タスク実行
task lint test
```

#### ベストプラクティス

1. **descを必ず記載**: `task`コマンドでタスク一覧を分かりやすく表示
2. **checkタスクを定義**: CI/CDと同じチェックをローカルで実行可能に
3. **varsで共通設定**: パスやコマンドを変数化して保守性向上

---

### pre-commit（Git Hooks）

[pre-commit](https://pre-commit.com/)はGitコミット前に自動でチェックを実行するフレームワークです。チーム全体で一貫したコード品質を維持できます。

#### インストール

```bash
uv add --dev pre-commit

# Git hooksをインストール
uv run pre-commit install
```

#### .pre-commit-config.yaml設定（Kaggle向け軽量版）

```yaml
# Kaggle向け軽量pre-commit設定
# 実験速度を重視しつつ、最低限の品質とセキュリティを担保

repos:
  # 基本的なファイルチェック
  - repo: https://github.com/pre-commit/pre-commit-hooks
    rev: v5.0.0
    hooks:
      # 大きなファイルのコミット防止（データ/モデルの誤コミット防止）
      - id: check-added-large-files
        args: ['--maxkb=5000']

      # 秘密鍵の検出（Kaggle APIキー等の漏洩防止）
      - id: detect-private-key

      # YAMLの構文チェック
      - id: check-yaml
        args: ['--unsafe']

      # マージコンフリクトマーカーの検出
      - id: check-merge-conflict

  # Ruff (Linter + Formatter) - 自動修正されるので邪魔にならない
  - repo: https://github.com/astral-sh/ruff-pre-commit
    rev: v0.8.3
    hooks:
      - id: ruff
        args: [--fix]
      - id: ruff-format
```

**Kaggleでは含めないもの（実験速度優先）:**
- 型チェック（ty/mypy）- 実験段階では厳格すぎる
- import-linter - 小規模では不要
- nbstripout - ノートブック出力を残したい場合がある
- gitleaks - 必要に応じて追加

**本番/チーム開発では追加を検討:**
```yaml
  # シークレット検出
  - repo: https://github.com/gitleaks/gitleaks
    rev: v8.21.2
    hooks:
      - id: gitleaks

  # Jupyter notebook整理
  - repo: https://github.com/kynan/nbstripout
    rev: 0.8.1
    hooks:
      - id: nbstripout
```

#### 使用方法

```bash
# 全ファイルに対して実行
uv run pre-commit run --all-files

# 特定のhookのみ実行
uv run pre-commit run ruff --all-files

# hookを更新
uv run pre-commit autoupdate

# hookをスキップしてコミット（緊急時のみ）
git commit --no-verify -m "Emergency fix"
```

#### 推奨フック

| フック | 用途 |
|--------|------|
| `trailing-whitespace` | 行末の空白除去 |
| `end-of-file-fixer` | ファイル末尾の改行統一 |
| `check-added-large-files` | 大きなファイルのコミット防止 |
| `detect-private-key` | 秘密鍵のコミット防止 |
| `ruff` | Python linting + formatting |
| `gitleaks` | シークレット（APIキー等）検出 |
| `nbstripout` | Notebookの出力削除 |

#### ベストプラクティス

1. **プロジェクト初期に導入**: 後からの導入は既存違反の修正が大変
2. **autoupdateは慎重に**: 信頼できるリポジトリのみ使用
3. **CIでも同じチェック**: `pre-commit run --all-files`をCI/CDでも実行
4. **--no-verifyは最終手段**: 緊急時以外は使用しない

---

## 開発ワークフロー統合

### プロジェクトセットアップ

```bash
# 1. プロジェクト作成
mkdir kaggle-competition && cd kaggle-competition
git init
uv init
uv python pin 3.10

# 2. 開発ツールインストール
uv add --dev ruff ty pre-commit import-linter pytest

# 3. pre-commit設定
uv run pre-commit install

# 4. Task インストール（別途）
brew install go-task  # macOS
```

### 日常の開発フロー

```bash
# 1. コード変更
vim src/models/baseline.py

# 2. ローカルチェック（コミット前）
task check  # または個別に: task lint, task typecheck

# 3. コミット（pre-commitが自動実行）
git add .
git commit -m "feat: Add baseline model"

# 4. 訓練実行
task train -- --config configs/baseline.yaml
```

### CI/CD統合（GitHub Actions例）

```yaml
# .github/workflows/ci.yml
name: CI

on: [push, pull_request]

jobs:
  check:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Install uv
        uses: astral-sh/setup-uv@v4

      - name: Install Task
        uses: arduino/setup-task@v2

      - name: Setup Python
        run: uv python install

      - name: Install dependencies
        run: uv sync --all-extras --dev

      - name: Run all checks
        run: task check
```

---

## リソース・リファレンス

### 公式ドキュメント
- [Ruff](https://docs.astral.sh/ruff/) - Linter/Formatter
- [ty](https://docs.astral.sh/ty/) - 型チェッカー
- [import-linter](https://import-linter.readthedocs.io/) - アーキテクチャ検証
- [Task](https://taskfile.dev/) - タスクランナー
- [pre-commit](https://pre-commit.com/) - Git hooks

### Kaggle/ML関連
- [Kaggle API](https://github.com/Kaggle/kaggle-api)
- [timm](https://timm.fast.ai/)
- [PyTorch](https://pytorch.org/docs/)
- [trackio](https://github.com/gradio-app/trackio)
- [uv](https://github.com/astral-sh/uv)

---

## Kaggle環境情報（2025年1月時点）

- Python: 3.10.13
- PyTorch: 2.1.2 with CUDA 12.1
- GPU VRAM: 16GB
- 時間制限: GPUノートブック9時間

---

Last Updated: 2025-12-12
